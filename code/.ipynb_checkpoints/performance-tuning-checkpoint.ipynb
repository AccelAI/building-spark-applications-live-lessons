{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import json\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = ps.SparkContext(master='spark://127.0.0.1:7077', conf=ps.SparkConf().setAppName(\"Performance Tuning\"))\n",
    "hive_context = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# need to get local path since we are reading local files\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readychef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_rdd = sc.textFile('file://' + cwd + '/data/readychef/meals.txt')\n",
    "events_rdd = sc.textFile('file://' + cwd + '/data/readychef/events.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header_meals = meals_rdd.first()\n",
    "header_events = events_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_no_header = meals_rdd.filter(lambda row: row != header_meals)\n",
    "events_no_header = events_rdd.filter(lambda row: row != header_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_json = meals_no_header.map(lambda row: row.split(';')) \\\n",
    "                            .map(lambda row_list: dict(zip(header_meals.split(';'), row_list)))\n",
    "    \n",
    "events_json = events_no_header.map(lambda row: row.split(';')) \\\n",
    "                              .map(lambda row_list: dict(zip(header_events.split(';'), row_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meals_json.cache()\n",
    "events_json.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def type_conversion(d, columns):\n",
    "    for c in columns:\n",
    "        d[c] = int(d[c])\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_typed = meals_json.map(lambda j: json.dumps(type_conversion(j, ['meal_id', 'price'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_typed = events_json.map(lambda j: json.dumps(type_conversion(j, ['meal_id', 'userid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'italian', 22575),\n",
       " (u'french', 16179),\n",
       " (u'mexican', 8792),\n",
       " (u'japanese', 6921),\n",
       " (u'chinese', 6267),\n",
       " (u'vietnamese', 3535)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no pre-filter\n",
    "sc.setJobGroup('PySpark -- nofilter', \"PySpark nofilter join performance\")\n",
    "\n",
    "meals_json.keyBy(lambda x: x['meal_id']) \\\n",
    "            .join(events_json.keyBy(lambda x: x['meal_id'])) \\\n",
    "            .filter(lambda tup: tup[1][1]['event'] == 'bought') \\\n",
    "            .groupBy(lambda tup: tup[1][0]['type']) \\\n",
    "            .mapValues(lambda val: len(val)) \\\n",
    "            .sortBy(lambda g: g[1], ascending=False) \\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'italian', 22575),\n",
       " (u'french', 16179),\n",
       " (u'mexican', 8792),\n",
       " (u'japanese', 6921),\n",
       " (u'chinese', 6267),\n",
       " (u'vietnamese', 3535)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-filter before join\n",
    "sc.setJobGroup('PySpark -- prefilter', \"PySpark cached RDD join performance\")\n",
    "\n",
    "meals_json.keyBy(lambda x: x['meal_id']) \\\n",
    "            .join(events_json.filter(lambda x: x['event'] == 'bought') \\\n",
    "                             .keyBy(lambda x: x['meal_id']) \\\n",
    "                 ) \\\n",
    "            .filter(lambda tup: tup[1][1]['event'] == 'bought') \\\n",
    "            .groupBy(lambda tup: tup[1][0]['type']) \\\n",
    "            .mapValues(lambda val: len(val)) \\\n",
    "            .sortBy(lambda g: g[1], ascending=False) \\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'italian', 22575),\n",
       " (u'french', 16179),\n",
       " (u'mexican', 8792),\n",
       " (u'japanese', 6921),\n",
       " (u'chinese', 6267),\n",
       " (u'vietnamese', 3535)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "sc.setJobGroup('PySpark -- prefilter', \"PySpark cached RDD join with reduce by key performance\")\n",
    "\n",
    "meals_json.keyBy(lambda x: x['meal_id']) \\\n",
    "            .join(events_json.filter(lambda x: x['event'] == 'bought') \\\n",
    "                             .keyBy(lambda x: x['meal_id']) \\\n",
    "                 ) \\\n",
    "            .filter(lambda tup: tup[1][1]['event'] == 'bought') \\\n",
    "            .map(lambda tup: (tup[1][0]['type'], 1)) \\\n",
    "            .reduceByKey(lambda a, b: a + b) \\\n",
    "            .sortBy(lambda g: g[1], ascending=False) \\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.setJobGroup('Spark SQL', \"spark sql performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_dataframe = hive_context.jsonRDD(meals_typed)\n",
    "events_dataframe = hive_context.jsonRDD(events_typed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meals_dataframe.registerTempTable('meals')\n",
    "events_dataframe.registerTempTable('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(type=u'italian', cnt=22575),\n",
       " Row(type=u'french', cnt=16179),\n",
       " Row(type=u'mexican', cnt=8792),\n",
       " Row(type=u'japanese', cnt=6921),\n",
       " Row(type=u'chinese', cnt=6267),\n",
       " Row(type=u'vietnamese', cnt=3535)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which cuisine sells the most\n",
    "\n",
    "hive_context.sql(\"\"\"\n",
    "    SELECT type, COUNT(type) as cnt FROM\n",
    "        meals \n",
    "    INNER JOIN \n",
    "        events on meals.meal_id = events.meal_id\n",
    "    WHERE\n",
    "        event = 'bought'\n",
    "    GROUP BY\n",
    "        type\n",
    "    ORDER BY cnt DESC\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sc.setJobGroup('Airline Data', \"Airline Dataset\")\n",
    "\n",
    "link = 's3n://mortar-example-data/airline-data'\n",
    "airline = sc.textFile(link)\n",
    "\n",
    "airline_no_quote = airline.map(lambda line: line.replace('\\'', '').replace('\\\"', '').strip(','))\n",
    "\n",
    "header_line = airline_no_quote.first()\n",
    "header_list = header_line.split(',')\n",
    "\n",
    "airline_no_header = airline_no_quote.filter(lambda row: row != header_line)\n",
    "\n",
    "def make_row(row):\n",
    "    row_list = row.split(',')\n",
    "    \n",
    "    d = dict(zip(header_list, row_list))\n",
    "    \n",
    "    return d\n",
    "\n",
    "airline_rows = airline_no_header.map(make_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'12129', -6.7547169811320753), (u'15991', -6.0978441127694856), (u'12888', -5.9056603773584904), (u'14113', -5.3462002412545235), (u'10779', -5.1457627118644069), (u'13127', -5.0891265597147948), (u'14633', -4.9087677725118484), (u'10739', -4.666666666666667), (u'15897', -4.6107142857142858), (u'11274', -4.6034482758620694)]\n"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup('Airline Data', \"no filter\")\n",
    "\n",
    "destination_rdd = airline_rows.map(lambda row: (row['DEST_AIRPORT_ID'], row))\n",
    "origin_rdd = airline_rows.map(lambda row: (row['ORIGIN_AIRPORT_ID'], row))\n",
    "\n",
    "mean_delays_dest = destination_rdd.groupByKey() \\\n",
    "                                  .mapValues(lambda delays: \\\n",
    "                                             np.mean(map(lambda row: \\\n",
    "                                                             float(row['ARR_DELAY']) if row['ARR_DELAY'] else 0, \\\n",
    "                                                         delays.data)))\n",
    "    \n",
    "mean_delays_origin = origin_rdd.groupByKey() \\\n",
    "                               .mapValues(lambda delays: \\\n",
    "                                          np.mean(map(lambda row: \\\n",
    "                                                          float(row['DEP_DELAY']) if row['DEP_DELAY'] else 0, \\\n",
    "                                                      delays.data)))\n",
    "\n",
    "print mean_delays_origin.sortBy(lambda t: t[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'12129', -6.7547169811320753), (u'15991', -6.0978441127694856), (u'12888', -5.9056603773584904), (u'14113', -5.3462002412545235), (u'10779', -5.1457627118644069), (u'13127', -5.0891265597147948), (u'14633', -4.9087677725118484), (u'10739', -4.666666666666667), (u'15897', -4.6107142857142858), (u'11274', -4.6034482758620694)]\n"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup('Airline Data', \"filtered first\")\n",
    "\n",
    "destination_rdd = airline_rows.map(lambda row: (row['DEST_AIRPORT_ID'], \\\n",
    "                                               float(row['ARR_DELAY'] if row['ARR_DELAY'] else 0)))\n",
    "\n",
    "origin_rdd = airline_rows.map(lambda row: (row['ORIGIN_AIRPORT_ID'], \\\n",
    "                                          float(row['DEP_DELAY']) if row['DEP_DELAY'] else 0))\n",
    "\n",
    "mean_delays_dest = destination_rdd.groupByKey().mapValues(lambda delays: np.mean(delays.data))\n",
    "mean_delays_origin = origin_rdd.groupByKey().mapValues(lambda delays: np.mean(delays.data))\n",
    "\n",
    "print mean_delays_origin.sortBy(lambda t: t[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_rows.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11) PythonRDD[130] at RDD at PythonRDD.scala:43 []\n",
      " |   MapPartitionsRDD[122] at mapPartitions at PythonRDD.scala:346 []\n",
      " |   ShuffledRDD[121] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(11) PairwiseRDD[120] at groupByKey at <ipython-input-22-1acf67c3f725>:8 []\n",
      "    |   PythonRDD[119] at groupByKey at <ipython-input-22-1acf67c3f725>:8 []\n",
      "    |   MapPartitionsRDD[98] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "    |   s3n://mortar-example-data/airline-data HadoopRDD[97] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print mean_delays_origin.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'12402', (-6601.0, 6001)), (u'11898', (-5898.0, 1510)), (u'14113', (-4432.0, 829)), (u'14633', (-4143.0, 844)), (u'11648', (-3880.0, 1832)), (u'15991', (-3677.0, 603)), (u'12280', (-3646.0, 2664)), (u'10779', (-3036.0, 590)), (u'13127', (-2855.0, 561)), (u'11695', (-2795.0, 1892))]\n"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup('Airline Data -- filtered', \"reduceByKey + filtered\")\n",
    "\n",
    "destination_rdd = airline_rows.map(lambda row: (row['DEST_AIRPORT_ID'], \\\n",
    "                                               float(row['ARR_DELAY'] if row['ARR_DELAY'] else 0)))\n",
    "\n",
    "origin_rdd = airline_rows.map(lambda row: (row['ORIGIN_AIRPORT_ID'], \\\n",
    "                                          float(row['DEP_DELAY']) if row['DEP_DELAY'] else 0))\n",
    "\n",
    "mean_delays_dest = destination_rdd.mapValues(lambda row: (row, 1)) \\\n",
    "                                  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "mean_delays_origin = origin_rdd.mapValues(lambda row: (row, 1)) \\\n",
    "                               .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print mean_delays_origin.sortBy(lambda t: t[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[146] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_rows.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'12402', (-6601.0, 6001)), (u'11898', (-5898.0, 1510)), (u'14113', (-4432.0, 829)), (u'14633', (-4143.0, 844)), (u'11648', (-3880.0, 1832)), (u'15991', (-3677.0, 603)), (u'12280', (-3646.0, 2664)), (u'10779', (-3036.0, 590)), (u'13127', (-2855.0, 561)), (u'11695', (-2795.0, 1892))]\n"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup('Airline Data -- filtered', \"cache first run\")\n",
    "\n",
    "destination_rdd = airline_rows.map(lambda row: (row['DEST_AIRPORT_ID'], \\\n",
    "                                               float(row['ARR_DELAY'] if row['ARR_DELAY'] else 0)))\n",
    "\n",
    "origin_rdd = airline_rows.map(lambda row: (row['ORIGIN_AIRPORT_ID'], \\\n",
    "                                          float(row['DEP_DELAY']) if row['DEP_DELAY'] else 0))\n",
    "\n",
    "mean_delays_dest = destination_rdd.mapValues(lambda row: (row, 1)) \\\n",
    "                                  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "mean_delays_origin = origin_rdd.mapValues(lambda row: (row, 1)) \\\n",
    "                               .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print mean_delays_origin.sortBy(lambda t: t[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'12402', (-6601.0, 6001)), (u'11898', (-5898.0, 1510)), (u'14113', (-4432.0, 829)), (u'14633', (-4143.0, 844)), (u'11648', (-3880.0, 1832)), (u'15991', (-3677.0, 603)), (u'12280', (-3646.0, 2664)), (u'10779', (-3036.0, 590)), (u'13127', (-2855.0, 561)), (u'11695', (-2795.0, 1892))]\n"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup('Airline Data -- filtered', \"cache second run\")\n",
    "\n",
    "destination_rdd = airline_rows.map(lambda row: (row['DEST_AIRPORT_ID'], \\\n",
    "                                               float(row['ARR_DELAY'] if row['ARR_DELAY'] else 0)))\n",
    "\n",
    "origin_rdd = airline_rows.map(lambda row: (row['ORIGIN_AIRPORT_ID'], \\\n",
    "                                          float(row['DEP_DELAY']) if row['DEP_DELAY'] else 0))\n",
    "\n",
    "mean_delays_dest = destination_rdd.mapValues(lambda row: (row, 1)) \\\n",
    "                                  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "mean_delays_origin = origin_rdd.mapValues(lambda row: (row, 1)) \\\n",
    "                               .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print mean_delays_origin.sortBy(lambda t: t[1], ascending=True).take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
